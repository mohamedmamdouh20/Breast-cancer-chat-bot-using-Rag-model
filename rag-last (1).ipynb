{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rYwCVbrLJ4xr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-10-20T09:01:07.773790Z",
     "iopub.status.busy": "2024-10-20T09:01:07.773007Z",
     "iopub.status.idle": "2024-10-20T09:03:50.270545Z",
     "shell.execute_reply": "2024-10-20T09:03:50.269395Z",
     "shell.execute_reply.started": "2024-10-20T09:01:07.773737Z"
    },
    "id": "rYwCVbrLJ4xr",
    "outputId": "b91a4442-a103-49db-d67b-991a2cbd5fb4",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit \n",
    "\n",
    "!npm install localtunnel\n",
    "\n",
    "!pip install langchain langchain-community \n",
    "\n",
    "!pip install sentence-transformers \n",
    "\n",
    "!pip install chromadb \n",
    "\n",
    "!pip install transformers \n",
    "\n",
    "!pip install torch torchvision torchaudio \n",
    "\n",
    "!pip install huggingface_hub \n",
    "\n",
    "!pip install pypdf \n",
    "\n",
    "!pip install bitsandbytes \n",
    "\n",
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1352f950-a8a2-4a42-9f3f-55ead5540bf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T09:46:52.862304Z",
     "iopub.status.busy": "2024-10-20T09:46:52.861936Z",
     "iopub.status.idle": "2024-10-20T09:46:52.872353Z",
     "shell.execute_reply": "2024-10-20T09:46:52.871226Z",
     "shell.execute_reply.started": "2024-10-20T09:46:52.862269Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import streamlit as st\n",
    "from huggingface_hub import login\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "login(token=secret_value_0)\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"no_repeat_ngram_size\": 4,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "@st.cache_resource\n",
    "def load_pipeline():\n",
    "    model_id = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    return tokenizer, model\n",
    "\n",
    "@st.cache_resource\n",
    "def load_faiss():\n",
    "    input_dir = \"/kaggle/input/\"\n",
    "    pdf_data = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".pdf\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                data_load = PyPDFLoader(file_path)\n",
    "                data = data_load.load()\n",
    "                pdf_data.append(data)\n",
    "    \n",
    "    if not pdf_data:\n",
    "        st.error(\"No PDF files found in the specified directory.\")\n",
    "        return None, None, None\n",
    "\n",
    "    page_contents = [doc for i in range(len(pdf_data)) for doc in pdf_data[i]]\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=300)\n",
    "    documents = text_splitter.split_documents(page_contents)\n",
    "\n",
    "    embedder = SentenceTransformer('distiluse-base-multilingual-cased-v2')\n",
    "    pdf_embeddings = embedder.encode([doc.page_content for doc in documents])\n",
    "\n",
    "    dimension = pdf_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    \n",
    "    index.add(pdf_embeddings)\n",
    "    \n",
    "    return index, embedder, documents\n",
    "\n",
    "# Load the model and FAISS index\n",
    "tokenizer, model = load_pipeline()\n",
    "index, embedder, documents = load_faiss()\n",
    "st.session_state.index = index\n",
    "st.session_state.embedder = embedder\n",
    "st.session_state.documents = documents\n",
    "\n",
    "st.title(\"Medical ChatBot\")\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# Display chat messages from history on rerun\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "def query_faiss(query):\n",
    "    query_embedding = st.session_state.embedder.encode([query])\n",
    "    Dis, Ind = st.session_state.index.search(query_embedding, 10)\n",
    "    retrieved_docs = [st.session_state.documents[i].page_content for i in Ind[0]]\n",
    "    return retrieved_docs\n",
    "\n",
    "def format_chat_template(question, retrieved_context):\n",
    "    chat_template = f\"\"\"\n",
    "Context:\n",
    "{retrieved_context}\n",
    "\n",
    "Instructions:\n",
    "\"You are a helpful assistant that is an expert at extracting the most useful information from the given context above. Also bring in extra relevant infromation to the user query from outside the given context if there any.\"\n",
    "\"if there any refrences or links in the context just ignore it\"\n",
    "\"Answer with the same language as the question\"\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\n",
    "\"\"\"\n",
    "    return chat_template\n",
    "\n",
    "def get_response(query):\n",
    "    retrieved_docs = query_faiss(query)\n",
    "    if not retrieved_docs:\n",
    "        return \"Sorry, I couldn't retrieve any relevant documents.\"\n",
    "    retrieved_context = \" \".join(retrieved_docs)\n",
    "    formatted_input = format_chat_template(query, retrieved_context)\n",
    "    inputs = tokenizer(formatted_input, return_tensors=\"pt\").to(model.device)\n",
    "    output = model.generate(**inputs, **generation_args)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    answer_start = response.find(\"Answer:\") + len(\"Answer: \")\n",
    "\n",
    "    only_answer = response[answer_start:].strip()\n",
    "    \n",
    "    cleaned_answer = re.sub(r'\\s+', ' ', only_answer)\n",
    "    cleaned_answer = re.sub(r'[^\\w\\s.,;\\[\\]\\(\\)]', '', cleaned_answer)\n",
    "    sentences = re.findall(r'.*?[.!?]', cleaned_answer)\n",
    "    if sentences:\n",
    "        cleaned_answer = ' '.join(sentences)\n",
    "    \n",
    "    return cleaned_answer\n",
    "\n",
    "if prompt := st.chat_input(\"Ask Anything...\"):\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "\n",
    "    with st.spinner(\"Thinking...\"):\n",
    "        response = get_response(prompt)\n",
    "        \n",
    "    with st.chat_message(\"assistant\"):\n",
    "        st.markdown(response)\n",
    "\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bMa4EjAdK229",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-10-20T09:46:53.523089Z",
     "iopub.status.busy": "2024-10-20T09:46:53.522237Z"
    },
    "id": "bMa4EjAdK229",
    "outputId": "45daaf95-1426-4e32-a25c-65688b90fe23",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!streamlit run /kaggle/working/app.py  & npx localtunnel --port 8501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcce7056-ec8a-495d-9d52-377a179843f4",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5788854,
     "sourceId": 9510311,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5821761,
     "sourceId": 9554390,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5888613,
     "sourceId": 9642854,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 463.918656,
   "end_time": "2024-10-17T16:46:15.274137",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-17T16:38:31.355481",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
